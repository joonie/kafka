## 카프카 운영



- 명령어

  - 토픽 생성

  - ```sh
    > kafka-topics.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --replication-factor 1 --partitions 1 --topic peter-topic --create
    ```

  - 토픽 리스트 확인

    - 카프카에 만들어져 있는 토픽 리스트 확인

    ```sh
    > kafka-topics.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --list
    ```

  - 토픽 상세보기

    - 토픽의 파티션수가 궁금하거나 리더가 궁금한 경우

  - ```sh
    > kafka-topics.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --describe
    ```

  - 토픽 설정 변경

  - ```SH
    > kafka-configs.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --alter --entity-type topics --entity-name peter-topic --add-config retention.ms=3600000
    ```

    - --alter 추가, --entity-type에 topics, --entity-name에 토픽의 이름, --add-config에 보관주기 1시간 세팅

    - 만약에 보관주기 1시간 설정을 삭제하고 싶은 경우 --add-config 대신 --delete-config를 추가하고 옵션 --add-config에 retenttion.ms 를 넣으면 된다

    - ```sh
      > kafka-configs.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --alter --entity-type topics --entity-name peter-topic --delete-config retention.ms
      ```

  - 토픽의 파티션 수 변경

    - 주의 : 토픽의 파티션 수를 늘릴순 있지만 줄일수는 없다, 파티션만 늘리면 안되고 이에 맞게 컨슈머의 숫자도 늘려줘야한다

    - ```sh
      > kafka-topics.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --alter --topic peter-topic --partitions 2
      ```

  - 토픽의 리플리케이션 팩터(복제 수) 변경

    - json파일(/usr/local/kafka/rf.json) 생성

    - ```json
      {
      "version":1,
      "partition":[
      	{"topic":"peter-topic", "partition":0, "replicas":[1,2]},//파티션 0번은 1,2임으로 리플리카펙터가 2이고 앞의 숫자는 리더를 의미하기 때문에 브로커1이 리더, 브로커2가 리플리카라는 의미다
      	{"topic":"peter-topic", "partition":1, "replicas":[2,3]},//파티션 1번은 2,3임으로 이프리카펙터가 3이고 앞의 숫자는 리더를 의미하기 때문에 브로커2가 리더, 브로커3이 리플리카라는 의미다
        
       /**
       만약 리플리카팩터를 3으로 세팅했다면(총 4대) 파티션을 3개로 구성하고 [1,2,3], [2,3,4], [3,4,5]가 되야한다.
       **/
      ]
      }
      ```

    - 주의할 점

      - <u>Replicas의 첫번째 숫자</u>를 현재 상태의 토픽 파티션 정보를 확인한 후 <u>각 파티션의 현재 리더정보</u>와 <u>일치</u>하도록 설정해 파티션의 리더가 변경되지 않도록 해야 한다. 리더가 변경되지 않기 때문에 토픽의 리플리케이션 팩터를 변경해도 프로듀서와 컨슈머에 영향을 주지 않을 수 있는 것이다.

    - ```sh
      > kafka-reassign-partitions.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --reassignment-json-file /usr/local/kafka/rf.json --execute
      ```

    - ```sh
      > kafka-topics.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --describe
      
      > Topic:peter-topic	PartitionCount:2	ReplicationFactor:2	configs:
      Topic: petertopic Partition:0	Leade:1	Replicas:1,2	Isr:1,2
      Topic: petertopic Partition:1	Leade:1	Replicas:2,3	Isr:2,3
      
      ```

      

  - 컨슈머 그룹 리스트 확인

    - ```sh
      kafka-consumer-groups.sh --bootstrap-server peter-kafka001:9092,peter-kafka002:9092,peter-kafka003:9092 --list
      ```

  - 컨슈머 그룹 상세조회(현재의 컨슈머 상태와 오프셋 확인)

    - ```sh
      kafka-consumer-groups.sh --bootstrap-server peter-kafka001:9092,peter-kafka002:9092,peter-kafka003:9092 --group peter-consumer --describe
      ```

  - 주키퍼 스케일 아웃 (3대 -> 5대)

    - 절차

      1. 각 주키퍼 서버에 접속해 /data/myid를 추가한다

      2. ```sh
         echo "4" > /data/myid
         ```

      2. 주키퍼 환경설정 zoo.cfg 파일에 4,5번 주키퍼 서버의 호스트와 포트를 적는다

      3. ```sh
         tickTime=2000
         initLimit=10
         syncLimit=5
         dataDir=/data
         clientPort=2181
         server.1=peter-zk001:2888:3888 #server.숫자(숫자는 myid)
         server.2=peter-zk002:2888:3888 
         server.3=peter-zk003:2888:3888
         server.4=peter-zk004:2888:3888
         server.5=peter-zk005:2888:3888
         
         ```

      3. Zoo.cfg 파일을 생성하고 systems 설정까지 완료한 후 주키퍼를 실행한다. 

         > 주의
         >
         > 4~5번 주키퍼에는 1~5번의 설정이 모두 포함되어있는 반면,
         >
         > 1~3번 주키퍼에는 아직까지 1~3번만 설정되어있다.

      4. 팔로워부터 재시작해야하며 리더를 마지막에 재시작하자

         1.  /usr/local/zookeeper/bin/zkServer.sh status 로 리더/팔로워 여부 확인
         2. systemd 명령어 (Systemctl restart zookeeper-server.service) 로 주키퍼 재시작

      5. 5대의 주키퍼 앙상블이 잘 동작하고 있는 지 확인

         ```sh
         > echo mntr | nc localhost 2181 | grep zk_synced_followers
         
         zk_synced_followers	4
         ```

  - 카프카 스케일 아웃

    - 카프카 설정 파일에서 broker.id 부분만 다른 서버와 겹치지 않게 추가한 후 실행하면 자동으로 클러스터에 추가된다

    - 파티션 수 5개, 리플리케이션 책터 2인 peter5라는 토픽을 추가한다

    - ```sh
      > kafka-topics.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --replication-factor 2 --partitions 5 --topic peter5 --create
      ```

    - 카프카 브로커 추가 (peter-kafka004, peter-kafka005)

    - server.properties 에 peter-kafka004는 broker.id=4로 입력하고 peter-kafka005는 broker.id=5로 입력한다

    - 주키퍼 입력도 기존 브로커에 설정한 정보와 동일하게 설정한다

    - 카프카를 실행한다

    - ```sh
      > systemctl start kafka-server.service
      ```

    - 클러스터에 잘 조인되었는지 주키퍼에서 확인하자

      - 주키퍼 cli에 접속 (/usr/local/zookeeper/bin/zkCLi.sh)
      - ls로 정보 확인 (ls /peter-kafka/brokers/ids)

    - 지금 추가한 브로커에도 바로 처리에 투입될 수 있도록 파티션 분산작업을 해야한다

      - peter5 토픽이 어떻게 처리되고 있는지 확인

      - ```sh
        > kafka-topics.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --topic peter5 --describe
        
        #1~3번 브로커에만 분산되어있음.
        > Topic:peter5	PartitionCount:5	ReplicationFactor:2	configs:
        Topic: peter5 Partition:0	Leader:2	Replicas:2,1	Isr:2,1
        Topic: peter5 Partition:1	Leader:3	Replicas:3,2	Isr:3,2
        Topic: peter5 Partition:2	Leader:1	Replicas:1,3	Isr:1,3
        Topic: peter5 Partition:3	Leader:2	Replicas:2,3	Isr:2,3
        Topic: peter5 Partition:4	Leader:3	Replicas:3,1	Isr:3,1
        
        ```

      - Partition.json(/usr/local/kafka/partition.json) 파일생성

        - ```json
          {
          "version":1,
          "partition":[
          	{"topic":"peter5", :partition":0, "replicas:[2,1]"},
          	{"topic":"peter5", :partition":1, "replicas:[3,2]"},
          	{"topic":"peter5", :partition":2, "replicas:[4,3]"},
          	{"topic":"peter5", :partition":3, "replicas:[5,4]"},
          	{"topic":"peter5", :partition":4, "replicas:[1,5]"},
          ]
          }
          ```

      - Partition.json 파일을 Kafka 하위 경로(/usr/local/kafka/partition.json)에 만들어 놓고 리밸런스를 실행한다.

      - ```sh
        > kafka-reassgin-partitions.sh --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka --reassignment-json-file /usr/local/kafka/partition.json --execute
        ```

        

      - 주의) 파티션의 크기가 크다면(10G 가량..) 네트워크 사용량을 급증시켜 브로커에게 상당한 부담이 될 수 있다
        - 안전하게 파티션 분산으로 하려면
          - 토픽의 사용량이 적은 시간대에 한다
          - 토픽의 보관주기를 줄여놔야한다